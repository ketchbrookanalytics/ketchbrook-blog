[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Don’t be a stranger – we would love to hear from you!\nGet in touch with us at info@ketchbrookanalytics.com today."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html",
    "title": "Taming Text-to-SQL",
    "section": "",
    "text": "Have you ever wished that you could ask your data questions in plain English?\nAt Ketchbrook Analytics, we made this a reality for our clients – and it’s completely changed the way they consume data. In this blog post, we have decided to peel back the curtain on our journey to building this solution, which we are coming up on the 1 year anniversary of launching. We’ll share what worked well, what didn’t, and why the end result has been so successful."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#core-architecture",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#core-architecture",
    "title": "Taming Text-to-SQL",
    "section": "Core Architecture",
    "text": "Core Architecture\nOur first step was to research the State of the Art (SOA) in Text-To-SQL and implementation schematics (and we encourage you to do so as well if you plan on following our steps, as SOA is advancing rapidly nowadays). For large databases, many solutions basically tend to go something like this:\n\nFirst, you’ll need to set up a workflow that extracts the source database’s schema (tables, columns, keys, joins, etc.) and as much metadata as possible. We’ve named it “DDL” (Data Definition Language) in our graph above, since most of the output of that process is comprised by DDL statements. The workflow then splits the DDL into smaller chunks and runs those chunks through an embedding model (which turns text into numbers) then saves the embedded result in a vector database.\nWe can also automate this workflow so that it can continuously run on a set schedule to keep the vector database up to date (i.e., so that user questions are as relevant as possible with respect to changes made to the source database over time).\nThe next step is to set up a second workflow that leverages the vector database created in the first workflow. Once the user prompt is received, we must develop a pipeline to retrieve the pertinent DDL that should accompany it as context for the LLM. To do so, we embed the query and use semantic/hybrid search in our vector database to get the information that would help us answer the user’s question (i.e. if the user prompt is “How many employees do we have?”, we’ll likely need information about the “Employee” table in the source database).\nBoth the initial user query and the pertinent DDL are then fed to the LLM as a prompt, as well as our request that the LLM return SQL code that can answer the query. The LLM spits out our SQL, we execute it against the source database, fetch the results, and display them to the user.\nHopefully this initial framework we’ve described helps set the stage, but there are certainly many more questions to answer: Which LLM should I use? Which embedding model? Which Vector DB? Where will I stand all of this up? What are the hardware requirements? The cost? If we are depending on RAG, how consistent will the performance be? And many more questions that you may already be asking yourself."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#choosing-the-engine",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#choosing-the-engine",
    "title": "Taming Text-to-SQL",
    "section": "Choosing the Engine",
    "text": "Choosing the Engine\nWhen talking about LLMs there are always two clear paths available:\n\nUse a third-party (OpenAI, Anthropic, Google)\nSelf-host with open-source models (Llama, DeepSeek, Gemma)\n\nThe second option (self-hosting your own LLM) gives you total control over cost and privacy. You know exactly what goes in and out, what gets stored and what doesn’t, where that storage is, and what other services/networks you let it communicate with. You can also use it as much as you want without worrying about running out of credits, running up a large bill, or dealing with vendor outages/downtime.\nThe downsides to self-hosting include cost and setup complexity. Hosting an LLM typically requires beefy hardware, and that can be expensive – especially if you are running it 24/7. It will also likely increase your latency because the costs of using a big enough server that responds in a second or two seconds instead of five or ten seconds can be prohibitive.\nA second minor downside to self-hosting is that third-party models tend to be the “best of the best”, while self-hosting relies on using open-source models that are slightly behind the latest third-party offerings.\nAll mentioned downsides for self-hosting are of course, upsides for the using third-party models. With third-party LLMs, you only pay for what you use (which can be as cheap as $0.01 per prompt/query). They tend to send back quicker responses because they are backed by massive data centers.\nThe main drawback for most organizations considering using third-party LLMs is the lack of ensured privacy. While many of the third-party vendors in this space offer paid subscriptions that come with “privacy-ensured” SLAs, most of these same vendors trained their models on copyrighted data (on everything they could get a hold of, really), which has raised significant ethical and legal concerns. Many firms we work with to stand up self-hosted LLM solutions appreciate the certainty that whatever they send to their model will not leave their environment."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-1-performance-and-cost",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-1-performance-and-cost",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 1: Performance and Cost",
    "text": "Challenge 1: Performance and Cost\n\nProblem\nOf course, the first problem is directly related to what we’ve just been talking about. Third party or self-host?\n\n\nSolution\nAfter initially pursuing a self-hosted approach, we quickly switched to a third-party LLM; primarily because the information that we would be exposing to our LLM was publicly available data. Further, it’s important to remember that the information we expose to the LLM was limited to only the schema of the source database; not the data in the database itself (i.e., we only provided metadata). Using a third-party model helped reduce both latency and cost.\nFor the specific third-party model, we chose Anthropic’s Claude, as we have consistently found it to be the best LLM for generating accurate SQL code from plain-English prompts. You should do your own research on models (and prices and rate limits) at when choosing one, given that the rankings change almost weekly and it wouldn’t make much sense for me to recommend a specific one."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-2-schema-complexity",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-2-schema-complexity",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 2: Schema Complexity",
    "text": "Challenge 2: Schema Complexity\n\nProblem(s)\nIn our case, all of the table names and columns were uppercase. This was a database design decision based upon matching the convention used by the FCA in the published data. Anyone who has worked with Postgres as their database engine knows that Postgres is not a fan of uppercase characters. While not catastrophic, this dichotomy reared its ugly head in that all column and table references in our generated SQL query would need to be encased in quotes; should you forget to quote them, Postgres would have a fit. LLMs tend to like fuzzy inputs and fuzzy outputs. They are non-deterministic, and forgetting a quote here and there would not be odd. Plus, they are not trained specifically on Postgres so they might get confused along the way and use a flavor of SQL meant for Oracle or Microsoft, for example\nAnother challenge was that both table and column names were acronyms and initialisms. You can’t expect an LLM (or a human really) to know what ACTRTFV means out of the gate without any context. If you read ROI, you might think Return of Investment which is a very common usage, but it might also mean Region of Interest or Radius of Influence (less common, but also used).\n\n\nSolution\nOur solution to these problems was two-fold. First, we reduced the scope and replaced all necessary tables with a single view that had what we thought was the most important information for our users. This also greatly affected our initial solution architecture, as we now don’t really need RAG for the DB schema if it will only have a single view. This removed our issues with table names, number of tables and joins, but we still had uppercase column names to deal with.\nTo solve that last part, we developed a script that uses the column’s metadata to change the column names from say “ROI” to “return_on_investment”. This is much more readable for any human (and when working with LLMs, it is a good practice to explain everything as you would a person that has no context, they can’t know details of your circumstances otherwise) or LLM, and it also lets them know which column the user is referring to in natural language."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-2a-aliases",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-2a-aliases",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 2a: Aliases",
    "text": "Challenge 2a: Aliases\n\nProblem\nOur solution from above generated another problem. Some users are very knowledgeable of the domain vocabulary, and it is normal for them to use the acronyms and initialisms in their queries to save time.\n\n\nSolution\nAdd the known aliases for column names to the prompt’s context."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-3-accuracy-and-performance",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-3-accuracy-and-performance",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 3: Accuracy and Performance",
    "text": "Challenge 3: Accuracy and Performance\n\nProblem\nA fair number of tests returned executable SQL that didn’t really answer our query, or misinterpreted it somehow. The performance was good, but not great, and not something that we’d be comfortable shipping to users.\n\n\nSolution\nAdding examples to the Prompt’s context window worked wonders on solving this issue. It gave the LLM a better sense of the underlying data, how to correctly interpret the users’ prompts, and what to expect.\nThe examples added were of varying SQL difficulty and contained a range of topics, but we tried to limit them somewhat. The objective is to add context but try to avoid the LLM just copying the example as-is; the user might want something similar but not exactly the same as the example we provided.\nPro Tip: In a solution incorporating RAG, you’ll likely want to pull examples for the table(s) selected during the vector search. Or inversely, get to the tables through the examples!\nThe examples contain a user’s natural language query and a resulting SQL query; they do not contain data."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-4-filter-columns",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-4-filter-columns",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 4: Filter columns",
    "text": "Challenge 4: Filter columns\n\nProblem\nYou’ll likely have columns that are prime targets for filtering. Some like dates and such don’t need extra work, but something like a specific company name might. Users can mistype (I do it alll the tiem), but the main issue is that they won’t likely type the field value exactly as it is saved on the database, especially if they don’t know how it was saved (and they shouldn’t have to!)\nYour SQL query might need “The Grand Fishing Company LLC.” to use as filter, otherwise it won’t work. Users are likely to write “grand fishing” at best.\n\n\nSolution\nThis is the only instance where we might share some of the underlying data with the LLM. Passing along a list of the available values for this type of field solves this issue to great effect, especially if you instruct the LLM to not deviate of that list. Users get the chance to mistype or partially complete as they like and the LLM will match with the correct one or do a best guess."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-5-context-window-limitations",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-5-context-window-limitations",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 5: Context window limitations",
    "text": "Challenge 5: Context window limitations\n\nProblem\nContext windows are finite and making them too big tends to result in bad performance. Nowadays most models accept a context window of about 120k tokens, but going over 32k starts making models unreliable with said context (at the time of writing, Google has launched a Gemini version that is supposed to accept 1M tokens as context and be surprisingly good with it, but we haven’t tested it yet) as you may run into the lost in the middle issue. This means that LLMs tend to pay attention to the beginning and end of the context, but not much to the middle of it.\nThere is also the issue of cost. If you are self-hosting your LLM, the cost is represented in latency and consumption of the available memory. Bigger context needs more RAM and so to handle a bigger context you also need beefier hardware, which is more expensive. If using a third party (like Claude), it translates directly into cost as they tend to charge by the number of inputted and outputted tokens.\n\n\nSolution\nOur solution to problem 2 greatly impacted this problem as well, as the resulting schema is small. We also spent a good amount of time prompt engineering to make sure we were efficient with our token usage while getting good performance, and we made sure the important parts of our instructions were at the beginning and end of our prompt.\nWe also decided to reduce the scope and make our tool translate single instances of queries. What I mean by this is that it will not work like chatting with ChatGPT where you have an interaction and the LLM remembers what you’ve been talking about. We have chat history added to the context, and it is a single-shot process. You input a natural language question; you get SQL and data as a result."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-6-error-handling",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#challenge-6-error-handling",
    "title": "Taming Text-to-SQL",
    "section": "Challenge 6: Error handling",
    "text": "Challenge 6: Error handling\n\nProblem\nEven with temperature set to 0, the LLM might sometimes return an answer with SQL that isn’t executable. It can be a syntax error, a SQL function that is meant for a different database, or some other hallucination it decided to spit out.\n\n\nSolution\nFirst, trying to execute the query against the database generates latency, not only due to connecting to the service and sending the query to the engine, but also because on most instances the query is likely executable, and we need to wait for the engine to gather the result and return it.\nWe need a way to check right away if the query can be executed and not waste so much precious time. We decided to generate an empty copy of the database in the same instance as our backend. That way, it is extremely lightweight, and we don’t even ask for an execution, we just ask the engine to “prepare” or “compile” the SQL query. This step involves parsing the query and checking its syntax against the database schema without actually fetching or modifying data.\nThen, if the results are bad, we capture the error and generate a second LLM call that contains some of the original context, the resulting query (the one we wanted to run and failed) and the resulting error, and we add instructions for it to attempt to fix it."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#technical-takeaways",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#technical-takeaways",
    "title": "Taming Text-to-SQL",
    "section": "Technical takeaways",
    "text": "Technical takeaways\n\nThe two main things you need to define are:\n\nIf you’ll self-host the LLM or utilize a third party API,\nHow you’ll provide the necessary context for the LLM (mainly a DDL that is good enough for it to generate a good prompt)\n\nAfter that, keep in mind that:\n\nAdding examples helps the LLM understand what to expect in its interactions with users, what expected results should look like, and how it should go about interpreting your DDL (just like a human would do)\nValue lists for filter fields are a must. Users shouldn’t have to know the exact wording they need to use for a proper SQL query, and they should be able to mistype or make mistakes occasionally\n\nYou need to plan for error recovery. LLMs are good at fuzzy input and fuzzy output, but we need non-fuzzy, structured, syntactically correct output. That means it might get it wrong sometimes and you must handle those situations to maintain an acceptable level of accuracy and performance.\nYou’ll need to spend a good amount of time prompt-engineering. Testing and retesting prompts, creating both simple and complicated scenarios to make sure your solution is production ready.\n\nHere is a short summary of how our prompt is currently structured as mentioned throughout several of the challenges previously described:\n\nThe first column is the ”happy path” and the other one is the error-recovery path (Base prompts are different, hence the difference in colour)\n\nOf course, selecting a good underlying LLM that suits your needs is also important, but nowadays many are good enough (they were rarer when we started our journey over a year ago, but they are advancing rapidly)"
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#project-insights",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#project-insights",
    "title": "Taming Text-to-SQL",
    "section": "Project insights",
    "text": "Project insights\n\nAsk for feedback from your users and involve them as soon as possible. Once we had something slightly sketched and partly functional, we gathered some key users and explained how we wanted to tackle this new feature. We showed how it would look, how it would respond, we made wireframes of the UI, we shared examples. They gave us example questions (prompts) that they would want to ask such a tool. We incorporated their valuable feedback into our final solution and it paid off in spades.\nIntegrate a mechanism for feedback. User feedback from the usage of your product is most important and it should be easy for the user to give it, and for you to use it. Also, however possible, incentivize your users to actually provide you with feedback – simply making a place for them to provide feedback is often not enough.\nLog and save important data! You need to be able to generate performance metrics and a way to review the process and improve it; you can’t do that without data."
  },
  {
    "objectID": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#the-power-of-llms",
    "href": "posts/2026-02-16-taming-text-to-sql/taming-text-to-sql.html#the-power-of-llms",
    "title": "Taming Text-to-SQL",
    "section": "The power of LLMs",
    "text": "The power of LLMs\nLLMs are constantly getting better, including improved ability to handle a wider range of tasks. Most notable current changes are involving Agents, which give LLMs the power to do stuff more than only respond with text/images/instructions.\nIncorporating LLMs into products and services can generate big rewards and streamline many processes, but not everything should be solved with them. Be conscientious but incorporate them if you can do so in a way that provides value. We believe this is only the beginning!"
  },
  {
    "objectID": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html",
    "href": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html",
    "title": "Making Small Data Big(ger)",
    "section": "",
    "text": "One of the caveats of working with your traditional vanilla machine learning algorithms (e.g., linear regression, logistic regression, decision trees, etc.) is that each case (row) in your training dataset must be independent. Let’s explore what exactly this means.\nThe way those algorithms learn is by looking at a bunch of cases: the circumstances surrounding each case, and what the outcome was for each case. The algorithm then tries to boil all of these cases down to a handful of rules that do a pretty good job at explaining how certain circumstances generally lead to particular outcomes."
  },
  {
    "objectID": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#applications-in-credit-risk",
    "href": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#applications-in-credit-risk",
    "title": "Making Small Data Big(ger)",
    "section": "Applications in Credit Risk",
    "text": "Applications in Credit Risk\nWhen we think about credit risk models, the cases are perhaps a bunch of historical loans in your portfolio where we know what the outcome was. To provide a simplified example, let’s suppose we are building a logistic regression model where the possible outcomes are (1) the loan was paid back in full or, (2) the borrower defaulted on the loan.\n\n\n\n\nLoan ID\nDebt Coverage Ratio\nCredit Score\nIndustry Outlook\nOutcome\n\n\n\n1001\n1.334\n711\nFair\nPaid in Full\n\n\n1002\n1.163\n760\nPoor\nDefault\n\n\n1003\n1.858\n740\nExcellent\nPaid in Full\n\n\n1004\n0.767\n724\nPoor\nPaid in Full\n\n\n1005\n1.346\n712\nAbove Average\nDefault\n\n\n1006\n1.966\n697\nExcellent\nPaid in Full\n\n\n1007\n1.502\n671\nExcellent\nDefault\n\n\n1008\n1.117\n743\nFair\nPaid in Full\n\n\n\n\n\nIn order to create the above dataset to train our model, we had to aggregate each loan into a single observation, so that each row represents a unique loan. Remember, each case in our training data must be independent; for us this means that we cannot have any loan appear more than once. There are many approaches to doing this aggregation which we won’t cover today, but for now just remember that the approach taken should be driven by what information will be available at the time of scoring a new loan.\nAggregation is Limiting\nWhen we take the step of aggregating our data into unique loan-level observations, we are naturally reducing the amount of data we have to work with. If you have tons of data, this isn’t an issue. But one issue we run into often is severe class imbalance in our outcome. In other words, we tend to have a lot more “Paid in Full” cases than we have “Default” cases.\n\n“Remember, each case in our training data must be independent; for us this means that we cannot have any loan appear more than once.”\n\nBut what if we didn’t have to satisfy that independence assumption? What if we didn’t have to aggregate our data? After all, the original data in our database probably looks a lot more like this:\n\n\n\n\nLoan ID\nDate\nDebt Coverage Ratio\nCredit Score\nIndustry Outlook\nStatus\n\n\n\n1001\n2021-06-01\n1.925\n749\nExcellent\nCurrent\n\n\n1001\n2021-07-01\n1.674\n705\nGood\nCurrent\n\n\n1001\n2021-08-01\n1.334\n711\nFair\nPaid in Full\n\n\n1002\n2021-02-01\n1.199\n764\nGood\nCurrent\n\n\n1002\n2021-03-01\n1.163\n760\nPoor\nDefault\n\n\n1003\n2021-09-01\n0.644\n800\nAbove Average\nCurrent\n\n\n1003\n2021-10-01\n2.654\n728\nGood\nCurrent\n\n\n1003\n2021-11-01\n1.858\n740\nExcellent\nPaid in Full\n\n\n\n\n\nThis type of data is sometimes referred to as “longitudinal” data, and represents observations of the same subject(s) over multiple points in time. In our case, the “subjects” are the unique loans. Clearly, the rows in this type of dataset are not independent, since we see the same loan appear more than once.\nWhat’s to be Gained\nSuppose the independence condition didn’t exist, and we could use this longitudinal data to build our logistic regression model. What would we gain by doing so?\n\n\nMore Data: For starters, we would have a lot more data! In situations where we don’t have a ton of data to begin with, each row of data we do have is really important. Especially when we have class imbalance in our data – we need as much information about “Default” loans as possible to help our model develop those general rules (and avoid overfitting).\n\nMore Signal: Second, we can give our model insight into a loan’s history in a way that we weren’t able to with our aggregated dataset. For example, it’s probably important to distinguish between a loan that defaulted after being on the books for 3 years versus one that defaulted after 3 months. You can think of this as incorporating an entire additional predictor variable into our model."
  },
  {
    "objectID": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#enter-multi-level-models",
    "href": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#enter-multi-level-models",
    "title": "Making Small Data Big(ger)",
    "section": "Enter: Multi-Level Models",
    "text": "Enter: Multi-Level Models\nLuckily for us data scientists, we know that we have a lot more tools in our toolbox than just the three algorithms mentioned at the beginning of this article. One suite of lesser-known algorithms we might explore are multi-level models.\nIf you haven’t heard of multi-level models, you may be familiar with “mixed effects” or “hierarchical” models. These three terms all refer to roughly the same thing. The big advantage of this type of model? Each case in your training data does not have to be independent. This means that we can use a dataset that looks a lot more like the second table above, as opposed to the (aggregated) first table.\nAnalogous Algorithms\nFortunately for us, a lot of the more traditional algorithms have multi-level analogs.\n\n\nAustin Powers Scene, “We’re Not So Different, You and I”\n\nIn fact, there are multi-level and mixed effects flavors of logistic regression that allow you to accommodate dependence between rows in your training data.\nIn our next blog post, we will dive deeper into the technical approaches to implementing these kinds of algorithms for building better credit risk models."
  },
  {
    "objectID": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#interested-in-learning-more",
    "href": "posts/2022-02-19-making-small-data-bigger/making-small-data-bigger.html#interested-in-learning-more",
    "title": "Making Small Data Big(ger)",
    "section": "Interested in Learning More?",
    "text": "Interested in Learning More?\nGet in touch with us today at info@ketchbrookanalytics.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Latest in Data Science",
    "section": "",
    "text": "Taming Text-to-SQL\n\n\nHow we leveraged LLMs to bridge the gap between users and data.\n\n\n\n\n\nFeb 16, 2026\n\n\nFranco Ferrero\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing New USDA Data Using Open Source Tools\n\n\nAnd how the Parquet file format can help.\n\n\n\n\n\nAug 13, 2023\n\n\nMichael Thomas\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Probability of Default in Real-Time\n\n\nAnd the many benefits of a forward-looking approach.\n\n\n\n\n\nMar 21, 2022\n\n\nMichael Thomas\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Small Data Big(ger)\n\n\nDon’t let your model choice shrink your data.\n\n\n\n\n\nFeb 19, 2022\n\n\nMichael Thomas\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#a-big-data-release-from-the-usda",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#a-big-data-release-from-the-usda",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "A Big (Data) Release from the USDA",
    "text": "A Big (Data) Release from the USDA\nWhen the USDA released its Crop Sequence Boundaries (CSB) data set last month, we knew it was huge news. What we didn’t realize immediately, however, was that the data would also be huge. In fact, as we set out to explore how this dataset could be useful (particularly to our Farm Credit System clients), simply trying to read this data in both Python and R took hours before maxxing out the memory on our laptops, resulting in quite a few “BSOD”s1.\n\nFor some more background, the CSB dataset contains “…estimates of field boundaries, crop acreage, and crop rotations across the contiguous United States.”2 In a nutshell, it shows what is planted where, and how that has changed over time.\n\n\n\n\n\nCSB data in action\n\nAfter downloading the geodatabase files from the Datasets section of the CSB home page and failing to successfully read them into R or Python, we needed to come up with an alternative strategy to analyze the data in these files without running out of memory. We decided to try to get the data into a different file format than geodatabase (.gdb), and use the current cutting-edge file format: Parquet (pronounced “par-kay”).\nWhy Bother?\nAs a consulting firm that does a lot of work in the ag finance space, we are always trying to stay ahead of the curve and discover unique ways to provide value to our clients.\nCrops are the primary output of (non-livestock) producers. Knowing what crops are planted where, and how the composition of those crops has changed over time may be useful for a few reasons:\n\nMarketing to existing customers or identifying prospective clients\nEvaluating concentration risk (is your ACA’s loan portfolio increasing/decreasing in its diversity of crops farmed?)\nIdentifying crop rotation trends over time for capital/insurance planning purposes\n\nTo further level-set, the audience for this post is:\n\nAnalysts, data scientists and business intelligence folks working in the Farm Credit system.\n\nLastly, for every licensed product that Ketchbrook develops (such as our {KAscore} R package), we aim to provide one open source solution (such as our {migrate} R package, or this open CSB data)"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#migrating-to-parquet",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#migrating-to-parquet",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "Migrating to Parquet",
    "text": "Migrating to Parquet\nWe know that the geodatabase file format that our data comes in won’t work for local analysis. The CSB homepage recommends using ArcGIS to analyse the 19.5 million unique polygons within the data set, but we prefer to use open source software instead of pay-for-play tools.\nGeoparquet is a newer file type that leverages the powerful Apache parquet columnar data storage format, which stores data in .parquet files for efficient loads, compresses well and decreases data size tremendously.3\nThe goal of the Geoparquet project is for .parquet to become the new standard file format in the geospatial world, replacing the shapefile.\nFor our purposes, we want to be able to read those 19 million polygons in the CSB data into our R session, so we decided to convert the geodatabase file into partitioned .parquet files. After some research, we discovered that the {geopandas} Python package provides the ability to read in only a specified set of rows of the data from a .gdb file, instead of having to read the entire thing into memory. While this approach is admittedly somewhat crude, it was the best that we could come up with – and it worked!\n\nCodeimport geopandas as gpd\n\n# Define path to the un-zipped .gdb file\ngdb_path = \"./National_Final_gdb/CSB1522.gdb\"\n\n# Define path to destination folder to land .parquet files\nparquet_dest = \"/some/folder/somewhere/\"\n\n# Read in \"chunks\" of the data from the .gdb file (1 million rows at a time), \n# and write the \"chunks\" out to separate .parquet files\nrow_start = 0\nrow_end = 1000000\n\n# Write out every 1MM rows in the data from the .gsb file to a new .parquet file\n# (this loop will work for up to 100MM rows of data; you could -- and probably \n# should -- do this in a `while` loop, instead)\nfor r in range(0, 100):\n    print(f'Writing rows {row_start}:{row_end}')\n    rows = slice(row_start, row_end)\n    geo = gpd.read_file(gdb_path, rows=rows)\n    # If there's no data in the data frame, exit the loop (don't write to .parquet)\n    if len(geo) == 0:\n        break\n    else:\n        path = parquet_dest + \"csb_\" + str(r) + \".parquet\"\n        geo.to_parquet(path=path)\n        row_start = row_end + 1\n        row_end = (r + 2) * 1000000\n\n\nThe last step involved reading in the new .parquet files and re-writing them out (to .parquet format again), this time partitioned by state, instead of by an arbitrary number of rows. By partitioning (i.e., creating separate .parquet files for each “partition”) the data this way, we can reap huge performance gains when reading the data into memory. Also, we imagine that end users of this data will likely be interested in analyzing crop sequence boundaries for a few particular states. Partitioning the data by state will lead to additional performance improvements, since only the .parquet files for the state(s) of interest will need to be computed on. We decided to switch to R for this step (and for the rest of the analysis).\n\nCode# Establish an {arrow} connection to the .parquet files partitioned every 1MM rows,\n# and re-write the data out to .parquet files partitioned by STATEFIPS code (State)\narrow::open_dataset(\"/some/folder/somewhere/\") |&gt; \n  arrow::write_dataset(\n    path = \"/some/other_folder/somewhere/\",\n    partitioning = \"STATEFIPS\",\n    format = \"parquet\"\n  )\n\n\n\n\n\n\n\n\nTipYou can use this dataset!\n\n\n\nKetchbrook Analytics has made this optimized geoparquet dataset available to the public, via an AWS s3 Bucket. Currently, we have only hosted the most recent CSB data (which contains an eight-year history of crop sequence boundaries, as of 2022), but we plan to migrate the additional (older) data to s3 soon.\n\n\nNow that the re-partitioned Parquet data has been made available in AWS, we can establish a connection between R and the s3 bucket. From there, we can list the first few files in the bucket.\n\nCodelibrary(arrow)\nlibrary(stringr)\n\n# Establish a connection to the AWS s3 bucket\nbucket &lt;- arrow::s3_bucket(\"ketchbrook-public-usda-nass-csb\")\n\n# List the paths to the first few .parquet files in the `year=2022` directory\nbucket$ls(path = \"year=2022\", recursive = TRUE) |&gt; \n    stringr::str_subset(pattern = \".parquet$\") |&gt; \n    head()\n\n[1] \"year=2022/STATEFIPS=01/part-0.parquet\"\n[2] \"year=2022/STATEFIPS=04/part-0.parquet\"\n[3] \"year=2022/STATEFIPS=05/part-0.parquet\"\n[4] \"year=2022/STATEFIPS=06/part-0.parquet\"\n[5] \"year=2022/STATEFIPS=08/part-0.parquet\"\n[6] \"year=2022/STATEFIPS=09/part-0.parquet\"\n\n\nNote that the files themselves are all named part-0.parquet, but the folder paths indicate the year and state (FIPS code) of the data in the corresponding file.\nWhen Lazy Becomes Efficient\nBefore actually reading in the data from the s3 bucket, we should first discuss best practices for working with these types of files.\nFirst, let’s get lazy (in the good way). When we combine the underlying Arrow specification behind Parquet with R’s dplyr functions, we can take advantage of lazy evaluation4. Lazy evaluation simply means that we build a recipe of what we want to do with the data set, but we only apply the recipe when we are ready to bring the transformed data into memory. In other words, we can filter our data set (on disk) by state, county, acreage, etc., before bringing it into our R environment."
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#data-preparation",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#data-preparation",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "Data Preparation",
    "text": "Data Preparation\nLet’s take our first peek at the data to see what we’re dealing with. However, instead of pulling in all of the data from the s3 bucket, let’s only pull in data from Hartford County, Connecticut.\nNote that the sfarrow package lets us parse our Parquet data directly into the popular geospatial sf object format. This will help us build maps and perform geospatial calculations downstream.\n\nCodelibrary(sfarrow)\nlibrary(dplyr)\n\n# This code should only take a few seconds to run -- try it yourself!\nraw &lt;- arrow::open_dataset(bucket) |&gt;\n  dplyr::filter(STATEFIPS == 9) |&gt;  # Connecticut\n  dplyr::filter(CNTYFIPS == \"003\") |&gt;  # Hartford county\n  sfarrow::read_sf_dataset()  # interpret `geometry` column as an `sf` object\n\n# List the columns, types, and first few values in each column\ndplyr::glimpse(raw)\n\nRows: 3,442\nColumns: 23\n$ CSBID        &lt;chr&gt; \"091522009778739\", \"091522009778745\", \"091522009778753\", …\n$ CSBYEARS     &lt;chr&gt; \"1522\", \"1522\", \"1522\", \"1522\", \"1522\", \"1522\", \"1522\", \"…\n$ CSBACRES     &lt;dbl&gt; 2.561646, 4.998030, 4.605023, 3.764729, 2.659328, 12.1047…\n$ R15          &lt;int&gt; 222, 176, 37, 37, 61, 222, 36, 61, 1, 36, 61, 36, 222, 22…\n$ R16          &lt;int&gt; 61, 37, 37, 37, 37, 61, 36, 61, 61, 36, 12, 36, 12, 61, 3…\n$ R17          &lt;int&gt; 11, 37, 37, 37, 37, 1, 37, 1, 1, 36, 27, 37, 1, 11, 36, 3…\n$ R18          &lt;int&gt; 11, 37, 37, 37, 37, 1, 37, 1, 37, 37, 1, 37, 1, 1, 36, 37…\n$ R19          &lt;int&gt; 11, 37, 37, 37, 176, 11, 1, 11, 37, 37, 61, 37, 12, 11, 3…\n$ R20          &lt;int&gt; 11, 37, 37, 37, 1, 11, 1, 11, 37, 37, 11, 37, 11, 11, 37,…\n$ R21          &lt;int&gt; 11, 176, 37, 37, 1, 11, 176, 11, 1, 37, 11, 37, 12, 11, 3…\n$ R22          &lt;int&gt; 11, 176, 37, 37, 1, 11, 176, 11, 1, 37, 11, 176, 43, 11, …\n$ STATEASD     &lt;chr&gt; \"0910\", \"0910\", \"0910\", \"0910\", \"0910\", \"0910\", \"0910\", \"…\n$ ASD          &lt;chr&gt; \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10…\n$ CNTY         &lt;chr&gt; \"Hartford\", \"Hartford\", \"Hartford\", \"Hartford\", \"Hartford…\n$ CNTYFIPS     &lt;chr&gt; \"003\", \"003\", \"003\", \"003\", \"003\", \"003\", \"003\", \"003\", \"…\n$ INSIDE_X     &lt;dbl&gt; 1895864, 1882887, 1884842, 1895255, 1896851, 1897364, 189…\n$ INSIDE_Y     &lt;dbl&gt; 2348512, 2345240, 2345735, 2348269, 2348659, 2348843, 234…\n$ Shape_Leng   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Shape_Length &lt;dbl&gt; 517.9627, 797.8349, 628.0148, 781.6467, 477.5219, 1433.17…\n$ Shape_Area   &lt;dbl&gt; 10366.61, 20226.31, 18635.87, 15235.32, 10761.92, 48986.1…\n$ year         &lt;int&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 202…\n$ STATEFIPS    &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, …\n$ geometry     &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((1895891 234..., MULTIPOLYGON…\n\n\nIn looking through the list of columns and first few rows of data in each column, it appears that there is a unique identifier for each field (CSBID), crop codes for a given year for that field (R15 … R22), and additional fields that describe the polygon (i.e., crop sequence boundary). The crop codes aren’t particularly useful to us; instead, we need to find out which actual crop each of those codes actually represents (i.e., we need a look-up table).\nUnfortunately, the crop codes metadata is stored in an ugly XML file, but our team took the time to create a crop codes lookup table in .csv format for your convenience.\n\nCodelibrary(knitr)\n\n# Read in the crop codes lookup table\nlookup &lt;- read.csv(\n  file = \"https://raw.githubusercontent.com/ketchbrookanalytics/usda-csb-data/main/data/crop_types.csv\",\n  colClasses = c(\"integer\", \"character\"),\n  strip.white = TRUE\n)\n\n# Take a look at the first few rows of the crop codes lookup table\nhead(lookup) |&gt; \n  knitr::kable(align = c(\"c\", \"l\"))\n\n\n\ncategorization_code\nland_cover\n\n\n\n0\nBackground\n\n\n1\nCorn\n\n\n2\nCotton\n\n\n3\nRice\n\n\n4\nSorghum\n\n\n5\nSoybeans\n\n\n\n\n\nNow, let’s replace the integer codes in the original CSB data with the descriptions from the look-up table.\n\nCode# Create a helper function to replace the crop code integer values with the \n# plain-English descriptions\nlookup_codes &lt;- function(var, codes) {\n\n  codes$land_cover[match(x = {{ var }}, table = codes$categorization_code)]\n\n}\n\n# Replace the integer codes in the crop rotation columns with the plain-English\n# descriptions of the cover\nclean &lt;- raw |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      .cols = tidyselect::matches(\"R[0-9][0-9]\"),\n      .fns = function(x) lookup_codes(var = x, codes = lookup)\n    )\n  ) |&gt; \n  # Rename the \"R*\" columns to the full year\n  dplyr::rename_with(\n    .fn = function(x) stringr::str_replace(x, \"R\", \"20\"),\n    .cols = tidyselect::matches(\"R[0-9][0-9]\")\n  )\n\n\nWe can take a look at how the crop cover changed for a few individual geometries (crop sequence boundaries) between 2020 and 2022.\n\nCodeclean |&gt; \n  sf::st_drop_geometry() |&gt; \n  dplyr::select(CSBID, `2020`:`2022`) |&gt; \n  dplyr::slice(6:10) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\nCSBID\n2020\n2021\n2022\n\n\n\n091522009778764\nTobacco\nTobacco\nTobacco\n\n\n091522009778766\nCorn\nGrassland/Pasture\nGrassland/Pasture\n\n\n091522009778773\nTobacco\nTobacco\nTobacco\n\n\n091522009778774\nOther Hay/Non Alfalfa\nCorn\nCorn\n\n\n091522009778776\nOther Hay/Non Alfalfa\nOther Hay/Non Alfalfa\nOther Hay/Non Alfalfa"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#data-visualization",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#data-visualization",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "Data Visualization",
    "text": "Data Visualization\nOnce we’ve calculated some summary statistics about our data and developed a few tables, a logical next step might be to start visualizing the data in a map or chart. We can employ some static and interactive maps and charts to help us gain a better understanding of the crop sequence boundary data.\nStatic Maps\nPerhaps you know that the {ggplot2} library is great for developing static charts, but did you know it can produce beautiful maps, too? Here’s a chart showing the crop type in each individual geometry (i.e., crop sequence boundary) in Hartford County, Connecticut.\n\nCodelibrary(ggplot2)\n\nclean |&gt; \n  ggplot2::ggplot(\n    ggplot2::aes(fill = `2022`)\n  ) + \n  ggplot2::geom_sf() + \n  ggplot2::labs(\n    title = \"Crop Cover Map (2022)\",\n    subtitle = \"Hartford County, Connecticut\"\n  ) +\n  ggplot2::theme(\n    legend.position = \"top\",\n    legend.title = ggplot2::element_blank()\n  )\n\n\n\n\n\n\n\nInteractive Maps\nWhile {ggplot2} is great for static visuals, {leaflet} maps are handy for interactive visualizations that can be embed in any HTML document or webpage. But before we make a {leaflet} map, let’s find out which crops are most prevalent in Hartford County.\n\nCodetop_3 &lt;- clean |&gt; \n  sf::st_drop_geometry() |&gt; \n  dplyr::count(Crop = `2022`, name = \"Count\", sort = TRUE) |&gt; \n  dplyr::slice_head(n = 3)\n\ntop_3 |&gt; \n  knitr::kable(\n    format.arg = list(NULL, big.mark = \",\"),\n    caption = \"Top 3 Crop Types in Hartford County (2022)\"\n  )\n\n\nTop 3 Crop Types in Hartford County (2022)\n\nCrop\nCount\n\n\n\nOther Hay/Non Alfalfa\n1,155\n\n\nCorn\n870\n\n\nGrassland/Pasture\n657\n\n\n\n\n\nThe top three types of crop are above. Let’s filter our dataframe to include only those, and eliminate extra fields we won’t need now.\n\nCodefor_leaflet &lt;- clean |&gt; \n  dplyr::filter(\n    `2022` %in% top_3$Crop\n  ) |&gt; \n  dplyr::select(\n    CSBID, \n    Crop = `2022`, \n    Shape_Area\n  ) |&gt; \n  # convert sq. meters to acres\n  dplyr::mutate(Shape_Area = round(Shape_Area / 4046.85642)) |&gt; \n  sf::st_transform(4326)\n\n\nOur data prep is done – time to make an interactive map!\n\nCodelibrary(leaflet)\n\n# Create separate data frames for each layer (crop) for our map\nhay &lt;- for_leaflet |&gt; \n  dplyr::filter(Crop == \"Other Hay/Non Alfalfa\")\n\ngrass &lt;- for_leaflet |&gt; \n  dplyr::filter(Crop == \"Grassland/Pasture\")\n\ncorn &lt;- for_leaflet |&gt; \n  dplyr::filter(Crop == \"Corn\")\n\n# Create palettes for each map\npal_hay &lt;- leaflet::colorNumeric(\n  palette = \"viridis\",\n  domain = hay$Shape_Area\n)\n\npal_grass &lt;- leaflet::colorNumeric(\n  palette = \"viridis\",\n  domain = grass$Shape_Area\n)\n\npal_corn &lt;- leaflet::colorNumeric(\n  palette = \"viridis\",\n  domain = corn$Shape_Area\n)\n\n# Create HTML popups\npopup_hay &lt;- paste0(\n  \"Crop: \", \"Other Hay/Non Alfalfa\", \"&lt;br&gt;\",\n  \"Acres: \", hay$Shape_Area\n)\n\npopup_grass &lt;- paste0(\n  \"Crop: \", \"Grassland/Pasture\", \"&lt;br&gt;\",\n  \"Acres: \", grass$Shape_Area\n)\n\npopup_corn &lt;- paste0(\n  \"Crop: \", \"Corn\", \"&lt;br&gt;\",\n  \"Acres: \", corn$Shape_Area\n)\n\n# Create the leaflet map\nleaflet::leaflet() |&gt; \n  leaflet::addProviderTiles(\n    provider = leaflet::providers$Esri.WorldTopoMap\n  ) |&gt; \n  leaflet::addPolygons(\n    data = hay, \n    fillColor = ~pal_hay(Shape_Area), \n    color = \"#b2aeae\", # you need to use hex colors\n    fillOpacity = 0.8, \n    weight = 1, \n    smoothFactor = 0.2,\n    group = \"Other Hay/Non Alfalfa\",\n    popup = popup_hay\n  ) |&gt; \n  leaflet::addPolygons(\n    data = grass, \n    fillColor = ~pal_grass(Shape_Area), \n    color = \"#b2aeae\", # you need to use hex colors\n    fillOpacity = 0.8, \n    weight = 1, \n    smoothFactor = 0.2,\n    group = \"Grassland/Pasture\",\n    popup = popup_grass\n  ) |&gt; \n  leaflet::addPolygons(\n    data = corn, \n    fillColor = ~pal_corn(Shape_Area), \n    color = \"#b2aeae\", # you need to use hex colors\n    fillOpacity = 0.8, \n    weight = 1, \n    smoothFactor = 0.2,\n    group = \"Corn\",\n    popup = popup_corn\n  ) |&gt; \n  leaflet::addLayersControl(\n    overlayGroups = c(\n      \"Other Hay/Non Alfalfa\",\n      \"Grassland/Pasture\",\n      \"Corn\"\n    ),\n    options = leaflet::layersControlOptions(collapsed = FALSE)\n  ) |&gt; \n  leaflet::hideGroup(c(\"Grassland/Pasture\", \"Corn\"))  |&gt; \n  leaflet::showGroup(\"Other Hay/Non Alfalfa\") |&gt; \n  leaflet::setView(lng = -72.552, lat = 41.992, zoom = 12)\n\n\nClick on a shape on the map to see the type of crop & acreage in each CSB\n\n\nFaceted Bar Charts\nMaybe we don’t care as much about the geometries within the data, but are more interested in the finding trends over time. For example, we could summarize the top five crops for the last four years in the data set. We can return back to {ggplot2} to make faceted bar charts that can illustrate these trends.\n\nCodelibrary(tidyr)\n\nclean |&gt; \n  sf::st_drop_geometry() |&gt; \n  tidyr::pivot_longer(\n    cols = starts_with(\"20\"),\n    names_to = \"crop_year\",\n    values_to = \"crop\"\n  ) |&gt; \n  dplyr::filter(\n    crop_year &gt;= 2019\n  ) |&gt; \n  dplyr::group_by(crop_year, crop) |&gt; \n  dplyr::summarise(\n    `Total Acres` = round(sum(Shape_Area)),\n    .groups = \"drop\"\n  ) |&gt; \n  dplyr::arrange(crop_year, dplyr::desc(`Total Acres`)) |&gt; \n  dplyr::slice_head(n = 5L, by = crop_year) |&gt; \n  ggplot2::ggplot(\n    ggplot2::aes(\n      x = reorder(crop, `Total Acres`),\n      y = `Total Acres`,\n      fill = crop\n    )\n  ) + \n  ggplot2::geom_col() + \n  ggplot2::scale_y_continuous(\n    labels = scales::label_comma(\n      scale = 1 / 1000000, \n      suffix = \"M\"\n    ),\n  ) +\n  ggplot2::scale_color_brewer(palette = \"viridis\") +\n  ggplot2::coord_flip() + \n  ggplot2::facet_wrap(~ crop_year, scales = \"free_y\") + \n  ggplot2::labs(\n    title = \"Top 5 Crops by Year\",\n    subtitle = \"Based upon Total CSB Area (in Acres)\"\n  ) + \n  ggplot2::theme(\n    axis.text.x = ggplot2::element_text(angle = 60, vjust = 1, hjust=1),\n    axis.title.y = ggplot2::element_blank(),\n    legend.position = \"none\",\n    panel.background = ggplot2::element_blank(),\n    panel.border = ggplot2::element_rect(fill = NA, color = \"black\")\n  )"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#whats-next",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#whats-next",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "What’s Next?",
    "text": "What’s Next?\nSo now that you know this data exists, where can you go from here?\nAnalyze the Data Yourself!\nWe encourage you to use our public .parquet files of this CSB data set. All you need to do is reference the s3 bucket like so:\n\nCodebucket &lt;- arrow::s3_bucket(\"ketchbrook-public-usda-nass-csb\")\n\n\nbefore using the framework in this article to query the data that is stored there.\nA Potential Use Case\nImagine you have another geospatial dataset containing locations of the fields (literal crop fields) a producer farms. A simple geospatial join with the CSB data would provide a wealth of information on what that producer is planting this year.\nAdditional Resources\nLastly, here are some other additional links that you may find helpful in your journey while analyzing the CSB data:\n\nThe GitHub repository developed by USDA to create the original CSB data is publicly available\nThe USDA NASS Quickstats database lets you query survey and census data related to agriculture. If you’d like to query this data from R, the {tidyUSDA} R package allows you to do just that, in addition to having built-in plotting functionality:\n\n\n\nPlotting with tidyUSDA R package"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#interested-in-learning-more",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#interested-in-learning-more",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "Interested in Learning More?",
    "text": "Interested in Learning More?\nGet in touch with us today at info@ketchbrookanalytics.com"
  },
  {
    "objectID": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#footnotes",
    "href": "posts/2023-08-09-geoparquet-for-usda-crop-maps/geoparquet-for-usda-crop-maps.html#footnotes",
    "title": "Analyzing New USDA Data Using Open Source Tools",
    "section": "Footnotes",
    "text": "Footnotes\n\nWe did successfully read the CSB data into QGIS without any blue screens, but this post is aimed at the R and Python crowd↩︎\nhttps://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/↩︎\nThis link contains a helpful intro to the geoparquet format.↩︎\nFor more information on Parquet and lazy evaluation, see our guest post on the Posit blog.↩︎"
  },
  {
    "objectID": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html",
    "href": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html",
    "title": "Estimating Probability of Default in Real-Time",
    "section": "",
    "text": "In our last blog post, we discussed some high-level strategies for getting more of your data into your model. This week we are going to dive deeper into the benefits of credit risk modeling frameworks that go beyond the “traditional” (linear/logistic regression, decision trees, etc.) approaches."
  },
  {
    "objectID": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#making-risk-more-dynamic",
    "href": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#making-risk-more-dynamic",
    "title": "Estimating Probability of Default in Real-Time",
    "section": "Making Risk More Dynamic",
    "text": "Making Risk More Dynamic\nLet’s envision a world where, instead of disparate models that assess probability of default at the time of certain credit events (i.e., one model for origination, another model for renewal, a third for CECL, etc.), we have a single model that estimates PD in real-time.\nThis world is not just possible – it’s a framework that Ketchbrook Analytics has spent years developing and building out for our banking clients.\nThe value proposition for this framework includes greater flexibility, decreased risk and improved insights. The technical details of the modeling approach allow any organization to:\n\nincorporate new information (new repayment data, new economic outlook data, etc.) about a loan to continuously update it’s predicted probability of default\nlook at its portfolio on any given day and understand the current risk for a single loan, a segment of loans, or the entire portfolio\nsimulate the impact of a new deal on the portfolio during the underwriting process\n\nFrom a modeling perspective, this means that we want our inputs (predictors) to be the longitudinal attributes about the loan, and the model output to be probability of default.\n\nWith each additional on-time payment, we would expect our model to decrease the associated probability of default. Conversely, if new economic outlook data forecasts worsening industry conditions, the model should update the probability of default to show increasing risk."
  },
  {
    "objectID": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#taking-a-forward-looking-approach",
    "href": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#taking-a-forward-looking-approach",
    "title": "Estimating Probability of Default in Real-Time",
    "section": "Taking a Forward-Looking Approach",
    "text": "Taking a Forward-Looking Approach\nEven more specifically, our model’s output should not be a single number – it should instead be a curve that provides us with not just a predicted probability of default, but a probability of default over the life of the loan.\nTake Loan #1234, for example. This five-year term loan just came into our bank today and passed through our model, which returned the following probability of default curve prediction:\n\n\n\n\n\n\n[Tip: You can hover over the chart above – it’s interactive! Each point represents the probability of the loan defaulting at or before that date. If we want to know the probability of a specific loan (including a new loan, a renewal, or a group of existing loans in a portfolio) defaulting today, we can easily get this by looking at the left-most point on the PD curve(s). If we want to know the probability of a loan ever defaulting during its life, we can easily get this by looking at the right-most point.]\nOur model estimates that Loan #1234 has a 0.8% probability of defaulting sometime prior to the end of this year (2022), a 2.0% probability of defaulting between now and the end of 2023, and so on, with a 16.0% probability of defaulting during its life.\nFast-forward two years later, and Loan #1234 has not yet defaulted, with three years remaining on its five-year term. We have new information about this loan’s performance that we did not have at origination (e.g., repayment history), as well as updated information (e.g., industry outlook). Running it through our model again, the curve has likely changed:\n\n\n\n\n\n\nIn the case of Loan #1234, it looks like it has been performing pretty well, since the updated probability of defaulting before term is down from 16.0% to 7.2%.\nOne Model to Rule Them All\nIn our toy example, we used an annual frequency along the x-axis of our charts. In practice, this frequency would be much more granular – at least monthly, and hopefully daily. Remember, the time granularity of our model’s output is tied directly to the frequency of the input data. In other words, if we get new repayment & economic outlook data each month, then (1) our model will output a predicted probability of default for each month into the future, and (2) that prediction curve can be updated each month.\n\nFrom a practical perspective, having a modeling framework that provides probability of default estimates in real-time (including origination) that updates dynamically as we get new information allows us to quickly build automated loan decisioning, scorecard lending, CECL, and stress testing models along the same probability of default framework."
  },
  {
    "objectID": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#interested-in-learning-more",
    "href": "posts/2022-03-21-estimating-probability-of-default-in-real-time/estimating-probability-of-default-in-real-time.html#interested-in-learning-more",
    "title": "Estimating Probability of Default in Real-Time",
    "section": "Interested in Learning More?",
    "text": "Interested in Learning More?\nGet in touch with us today at info@ketchbrookanalytics.com"
  }
]